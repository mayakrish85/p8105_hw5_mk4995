---
title: "P8105 Homework 5"
author: "Maya Krishnamoorthy"
date: "2024-11-13"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(purrr)
library(rvest)

set.seed(1)
```

## Problem 1

Suppose you put ùëõ people in a room, and want to know the probability that at least two people share a birthday. For simplicity, we‚Äôll assume there are no leap years (i.e. there are only 365 days) and that birthdays are uniformly distributed over the year (which is actually not the case).

```{r}
bday_sim = function(n) {
  
  bdays = sample(1:365, size = n, replace = TRUE)
  
  duplicate = length(unique(bdays)) < n
  
  return(duplicate)
}
```

```{r}
sim_res = 
  expand_grid(
    n = 2:50,
    iter = 1:10000
  ) |> 
  mutate(res = map_lgl(n, bday_sim)) |> 
  group_by(n) |> 
  summarize(prob = mean(res))

sim_res |> 
  ggplot(aes(x = n, y = prob )) + 
  geom_line() +
  labs(
    title = "Probability of Duplicate Birthdays as a Function of Group Size"
  )
```

The probability of having a duplicate birthday increases as the group size increases. For small group sizes, the probability of a duplicate birthday is close to zero. It starts off very low because with fewer people, the chance of having a matching birthday is small. As the group size approaches 50, the probability approaches 1 (or 100%) meaning that it is almost certain that there will be at least one pair of people that share a birthday.

## Problem 2

**Conduct a simulation to explore power in a one-sample t-test.**

Set initial variables.

```{r}
n = 30
sigma = 5
mu = 0
reps = 5000
alpha = 0.05
```

Generate 5000 datasets from the normal model. 

```{r}
sim_data <- tibble(
  id = integer(reps),
  sample = vector("list", reps)
)

for (i in 1:reps) {
  sim_data$id[i] = i
  sim_data$sample[[i]] = rnorm(n, mean = mu, sd = sigma)
}
```

Save mu-hat and the p-value evaluated from a t-test with a null hypothesis where `mu` = 0 using `alpha` = 0.05.

```{r}
results_df = 
  sim_data |> 
  mutate(
    mu_hats = map_dbl(sample, mean),
    t_test = map(sample, \(x) broom::tidy(t.test(x, mu = 0, conf.level = 0.95))),
    p_values = map_dbl(t_test, \(x) x$p.value)
  ) |> 
  select(id, mu_hats, p_values)
```

**Repeat the above for mu = {1,2,3,4,5,6}.**

Step 1: Create a function that evaluates for each value of mu.

```{r}
get_data = function(true_mean, n = 30, sigma = 5, reps = 5000) {
  # Create dataset of samples for each mu.
  sim_data <- tibble(
    id = integer(reps),
    sample = vector("list", reps)
  )
  
  for (i in 1:reps) {
    sim_data$id[i] = i
    sim_data$sample[[i]] = rnorm(n, mean = true_mean, sd = sigma)
  }
  
  # Calculate mu-hats and p-values per sample.
  results_df = 
  sim_data |> 
  mutate(
    mu_hats = map_dbl(sample, mean),
    t_test = map(sample, \(x) broom::tidy(t.test(x, conf.level = 0.95))),
    p_values = map_dbl(t_test, \(x) x$p.value)
  ) |> 
  select(id, mu_hats, p_values)
  
  return(results_df)
}
```

Step 2: Create a table for each value of mu.

```{r}
true_means = c(0, 1, 2, 3, 4, 5, 6)

final_df = 
  tibble(
    mu = true_means,
    results = map(true_means, \(x) get_data(true_mean = x))
  ) |> 
  unnest(results) |> 
  select(-id)
```


**Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of mu on the x axis.**

```{r}
final_df |> 
  group_by(mu) |>
  summarize(
    prop_rejected = mean(p_values < alpha)
  ) |>
  ggplot(aes(x = mu, y = prop_rejected)) +
  geom_point() +
  geom_line() + 
  labs(
    title = "True Mean vs. Power", 
    x = "True mean",
    y = "Power"
  )
```

As the size of the mean increases, the power of the test increases. This implies that as the effect size increases, so does the probability of correctly rejecting the null hypothesis (mu = 0). 

**Make a plot showing the average estimate of mu-hats for all tests, as well as for only rejected tests, on the y axis and the true value of mu on the x axis.**

```{r}
final_df |> 
  group_by(mu) |> 
  summarize(
    avg_estimate = mean(mu_hats),
    avg_rejected_estimate = mean(mu_hats[p_values < alpha])
  ) |> 
  ggplot(aes(x = mu)) +
  geom_line(aes(y = avg_estimate, color = "All Samples")) +
  geom_point(aes(y = avg_estimate, color = "All Samples")) +
  geom_line(aes(y = avg_rejected_estimate, color = "Null Rejected")) +
  geom_point(aes(y = avg_rejected_estimate, color = "Null Rejected")) +
  labs(
    title = "Average Estimate of Mu-hat vs. True Mu",
    x = "True mean",
    y = "Average mean estimate",
    color = "Sample Group"
  ) +
  scale_color_manual(
    values = c("All Samples" = "blue", "Null Rejected" = "purple")
  )
```

Across all samples, the average estimate is approximately equal to the true mean. The estimate for the sample population where the null was rejected is less likely to be equal to the true mean, which makes sense, because it represents the population where the mean estimate was significantly different from the true mean. Overall, though, as the effect size increases, the average estimate across both sample groups become approximately equal. 

## Problem 3

```{r}
library(readr)

url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_data = read_csv(url)

rows = nrow(homicide_data)
cols = ncol(homicide_data)
```

**Describe the data.**

The raw data describes the criminal homicides over a decade in 50 of the largest American cities. The dataset contains `r rows` rows and `r cols` columns. The variables include:
1. `uid`: unique identifier
2. `reported_date`: date
3. `victim_last`, `victim_first`: victim name
4. `victim_race`, `victim_age`, `victim_sex`: victim demographics
5. `city`, `state`, `lat`, `lon`: location of homicide
6. `disposition`: case status

**Create a city-state variable and an unsolved homicides variable.**

```{r}
homicide_data = 
  homicide_data |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    unsolved = ifelse(
      disposition %in% c("Closed without arrest", "Open/No arrest"), 1, 0
    )
  ) |> 
  filter(city_state != "Tulsa, AL") # Tulsa is not a city in AL - maybe authors intended Tuscaloosa? Only one case, so removed from dataset
```

**Summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is ‚ÄúClosed without arrest‚Äù or ‚ÄúOpen/No arrest‚Äù).**

```{r}
homicide_data_summarized = 
  homicide_data |> 
  group_by(city_state) |> 
  summarize(
    total_homicides = n(),
    total_unsolved_homicides = sum(unsolved)
  )

homicide_data_summarized |> 
  knitr::kable()
```

**For the city of Baltimore, MD, use the `prop.test` function to estimate the proportion of homicides that are unsolved.**

```{r}
homicide_data_baltimore = 
  homicide_data |> 
  filter(city_state == "Baltimore, MD") 

total_baltimore = nrow(homicide_data_baltimore)
total_unsolved_baltimore = sum(homicide_data_baltimore$unsolved)

prop_test_baltimore = prop.test(total_unsolved_baltimore, total_baltimore)

prop_test_baltimore |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high) |> 
  knitr::kable(digits=3)
```

**Now run prop.test for each of the cities in your dataset.**

```{r}
prop_homicides_city = 
  homicide_data_summarized |> 
  mutate(
    prop_test = map2(total_unsolved_homicides, total_homicides, ~ prop.test(.x, .y)),
    results = map(prop_test, broom::tidy)
  ) |> 
  unnest(results) |> 
  select(city_state, estimate, conf.low, conf.high)

prop_homicides_city |> 
  knitr::kable(digits=3)
```


**Create a plot that shows the estimates and CIs for each city.**

```{r}
prop_homicides_city_arranged =
  prop_homicides_city |> 
  arrange(desc(estimate)) |> 
  mutate(city_state = factor(city_state, levels = city_state))
  
prop_homicides_city_arranged |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  labs(
    title = "Estimated Proportion of Unsolved Homicides by City",
    x = "City, State",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```


